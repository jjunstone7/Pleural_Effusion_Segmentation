{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random \n",
    "import torch\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from totalsegmentator.libs import download_pretrained_weights\n",
    "import nibabel as nib\n",
    "from monai.networks.nets import BasicUNet\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed\n",
    "def seed_everything(random_seed = 42):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "# path\n",
    "dir_path = '/home/snuhub-user/workspace/updown_share/Pleural_CT/'\n",
    "\n",
    "# Hyper Parameter\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "Epochs = 75\n",
    "Learning_rate = 0.005\n",
    "Batch_size = 32\n",
    "Early_stopping_num = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor() # Future modifications and developments\n",
    "])\n",
    "train_augmentation = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip() # Future modifications and developments\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegDataset(Dataset):\n",
    "    def __init__(self, df, augmentation=None, preprocessing=None, train_mode=True):\n",
    "        self.df = df\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "        self.train_mode = train_mode\n",
    "        self.slices = []\n",
    "\n",
    "        if train_mode:\n",
    "            for i in tqdm(range(len(self.df))):\n",
    "                data_path = self.df.iloc[i, 0]\n",
    "                file_path = os.path.join(dir_path, 'data', 'SegCT_Anony10Percent', data_path)\n",
    "                image = nib.load(os.path.join(file_path, 'original.nii.gz'))\n",
    "                mask = nib.load(os.path.join(file_path, 'pleural_effusion.nii.gz'))\n",
    "                image_data = image.get_fdata()\n",
    "                mask_data = mask.get_fdata()\n",
    "\n",
    "                for j in range(int(image_data.shape[2])//2):\n",
    "                    self.slices.append((image_data[:, :, 2*j], mask_data[:, :, 2*j])) # Skip and extract one by one\n",
    "        else:\n",
    "            for i in tqdm(range(len(self.df))):\n",
    "                data_path = self.df.iloc[i, 0]\n",
    "                file_path = os.path.join(dir_path, 'data', 'SegCT_Anony10Percent', data_path)\n",
    "                image = nib.load(os.path.join(file_path, 'original.nii.gz'))\n",
    "                mask = nib.load(os.path.join(file_path, 'pleural_effusion.nii.gz'))\n",
    "                image_data = image.get_fdata()\n",
    "                mask_data = mask.get_fdata()\n",
    "\n",
    "                for j in range(int(image_data.shape[2])):\n",
    "                    self.slices.append((image_data[:, :,j], mask_data[:, :,j])) # Extract all \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, mask = self.slices[index]\n",
    "\n",
    "        mask = np.expand_dims(mask, axis=0)  # add Channel dimension \n",
    "        \n",
    "        if self.preprocessing:\n",
    "            img = self.preprocessing(img)\n",
    "            \n",
    "        mask_tensor = torch.from_numpy(mask).long()\n",
    "        \n",
    "\n",
    "        return img, mask_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =BasicUNet(spatial_dims=2, in_channels = 1, out_channels = 1, features=(32, 64, 128, 256, 512, 32))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_score(mask, target):\n",
    "    epsilon = 1e-8\n",
    "    intersection = torch.sum(mask * target, dim=[1, 2, 3])  # 각 배치 및 각 차원에 대해 합산\n",
    "    union = torch.sum(mask, dim=[1, 2, 3]) + torch.sum(target, dim=[1, 2, 3])\n",
    "    \n",
    "    dice = (2.0 * intersection + epsilon) / (union + epsilon)\n",
    "    \n",
    "    return dice.mean() # 차원을 평균 내기에 배치별 Dice 값이 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, train_dataset, valid_dataset, model):\n",
    "        self.epochs = 75\n",
    "        self.learning_rate = Learning_rate\n",
    "        self.batch_size = Batch_size\n",
    "        self.early_stopping_num = Early_stopping_num\n",
    "        \n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.valid_dataset = valid_dataset\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.valid_losses = []\n",
    "        self.val_dice_scores = []\n",
    "\n",
    "    def train(self):\n",
    "        train_loader = DataLoader(self.train_dataset, shuffle=True, batch_size=self.batch_size)\n",
    "        valid_loader = DataLoader(self.valid_dataset, shuffle=False, batch_size=22)\n",
    "        criterion = nn.BCELoss()  # Consider changing to Dice loss\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=25, eta_min=0.00001)\n",
    "\n",
    "        early_stopping_counter = 0 \n",
    "        top_dice_score = 0\n",
    "        top_valid_loss = 0\n",
    "        \n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "            self.model.train()\n",
    "            running_loss = 0\n",
    "            print(f'Epoch {epoch + 1} / {self.epochs}')\n",
    "            for _, data in tqdm(enumerate(train_loader), mininterval=30.0):\n",
    "                inputs, masks = data\n",
    "                inputs, masks = inputs.float().to(device), masks.float().to(device)\n",
    "                batch_size = inputs.size(0)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                outputs = torch.sigmoid(outputs)\n",
    "                loss = criterion(outputs, masks)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * batch_size\n",
    "                total_samples += batch_size\n",
    "                \n",
    "            train_loss = running_loss / total_samples\n",
    "            self.train_losses.append(train_loss)\n",
    "            print(f'Training loss: {round(train_loss, 4)}')\n",
    "            \n",
    "            self.model.eval()\n",
    "            val_loss_total = 0\n",
    "            val_score_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for _, val_data in enumerate(valid_loader):\n",
    "                    val_inputs, val_masks = val_data\n",
    "                    val_inputs, val_masks = val_inputs.float().to(device), val_masks.float().to(device)\n",
    "                    val_batch_size = val_inputs.size(0)\n",
    "                    \n",
    "                    val_outputs = self.model(val_inputs)\n",
    "                    val_outputs = torch.sigmoid(val_outputs)\n",
    "                    y_pred = (val_outputs > 0.5).float()\n",
    "                    val_score = dice_score(y_pred, val_masks)\n",
    "                    val_loss = criterion(val_outputs, val_masks)\n",
    "                    \n",
    "                    val_loss_total += val_loss.item() * val_batch_size\n",
    "                    val_score_total += val_score * val_batch_size \n",
    "                    total_val_samples += val_batch_size  \n",
    "\n",
    "            val_loss = val_loss_total / total_val_samples  \n",
    "            val_score = val_score_total / total_val_samples\n",
    "            self.valid_losses.append(val_loss)\n",
    "            self.val_dice_scores.append(val_score)\n",
    "            \n",
    "            \n",
    "            print(f'Dice Score: {val_score} | Valid Loss: {val_loss}')\n",
    "            \n",
    "            scheduler.step() # 스케쥴러 추가\n",
    "            \n",
    "            if val_loss > top_valid_loss:\n",
    "                top_valid_loss = val_loss\n",
    "                torch.save(self.model.state_dict(), f'/home/snuhub-user/workspace/updown_share/Pleural_CT/weights/top_second_epoch_{epoch}.pth')\n",
    "                early_stopping_counter = 0\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "            \n",
    "            if early_stopping_counter >= self.early_stopping_num:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    def plot(self):\n",
    "        epochs = range(1, len(self.train_losses) + 1)\n",
    "        plt.plot(epochs,self.train_losses, label='Training Loss')\n",
    "        plt.plot(epochs,self.valid_losses, label='Validation Loss')\n",
    "        val_dice_scores = [score.cpu().numpy() for score in self.val_dice_scores]\n",
    "        plt.plot(epochs,val_dice_scores, label='Validation Dice Score')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss/Dice Score')\n",
    "        plt.title('Loss/Score Visualization')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_raw = pd.read_excel(dir_path + 'PF_CT_10percent.xlsx')\n",
    "df_raw\n",
    "df_exist = df_raw[df_raw.Exist == 1].reset_index(drop=True)\n",
    "df_exist\n",
    "df_train, df_validtest = train_test_split(df_exist, test_size=25,stratify=df_exist.Bot_miss , random_state = 42)\n",
    "df_valid, df_test = train_test_split(df_validtest, test_size = 3,stratify=df_validtest.Bot_miss , random_state = 42)\n",
    "print(df_train.Bot_miss.value_counts())\n",
    "print(df_validtest.Bot_miss.value_counts())\n",
    "print(df_valid.Bot_miss.value_counts())\n",
    "print(df_test.Bot_miss.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SegDataset(df_train,train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = SegDataset(df_valid,train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(train_dataset, valid_dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boaz_mini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
